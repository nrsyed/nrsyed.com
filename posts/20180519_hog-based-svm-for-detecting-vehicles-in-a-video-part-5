This is the fifth post in a series on implementing an SVM object detection pipeline for video with OpenCV-Python.

<a href="https://nrsyed.com/2018/05/06/hog-based-svm-for-detecting-vehicles-in-a-video-part-1/">Part 1: SVMs, HOG features, and feature extraction</a>
<a href="https://nrsyed.com/2018/05/06/hog-based-svm-for-detecting-vehicles-in-a-video-part-2/">Part 2: Sliding window technique and heatmaps</a>
<a href="https://nrsyed.com/2018/05/10/hog-based-svm-for-detecting-vehicles-in-a-video-part-3/">Part 3: Feature descriptor code and OpenCV vs scikit-image HOG functions</a>
<a href="https://nrsyed.com/2018/05/16/hog-based-svm-for-detecting-vehicles-in-a-video-part-4/">Part 4: Training the SVM classifier</a>
Part 5: Implementing the sliding window search
<a href="https://nrsyed.com/2018/05/24/hog-based-svm-for-detecting-vehicles-in-a-video-part-6/">Part 6: Heatmaps and object identification</a>

The previous posts touched on the foundations of SVMs, HOG features, and the sliding window plus heatmap approach to find objects in an image, then discussed the code from portions of the pipeline responsible for extracting features from sample images and training an SVM classifier to differentiate between vehicles and non-vehicles. In this post, we'll discuss the portions of the code that implement the sliding window to determine which parts of an image contain an object. The Python source code and project <a href="https://github.com/nrsyed/svm-vehicle-detector" rel="noopener" target="_blank">are available on Github</a>.

As in the previous posts, let's start with a video of the final result to see what we're working towards:

<iframe width="800" height="450" src="https://www.youtube.com/embed/uOxkAF0iA3E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h2>Sliding window search</h2>
Although we've trained our SVM classifier on nice 64x64 images, an actual dashcam video will not be a 64x64 image that neatly contains or doesn't contain an entire vehicle. Instead, vehicles may be present in different parts of the image at different scales. One way to tackle this is with a sliding window search in which a window of fixed size is slid across and down the image. This is performed at several scales by scaling the image downâ€”this repeated downscaling is called an "image pyramid."

The results I obtained with the aforementioned approach were inconsistent, so I tried something different. My alternative approach was to 1) scan only the portion of the image below the horizon and above the dash, since there should be no vehicles outside this region, and 2) use a window of variable size that's relatively small near the horizon and larger toward the bottom of the image, the rationale being that vehicles near the horizon are farther away and will appear smaller whereas vehicles near the bottom of the image are closer and will appear larger. A sample demonstration of this approach can be seen in the following video:

<iframe width="800" height="450" src="https://www.youtube.com/embed/6e8XB-bKsZ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

The colors don't signify anything, they're simply meant to help differentiate adjacent windows from one another; I wrote a simple little utility called <a href="https://github.com/nrsyed/utilities/blob/master/uniquecolors.py" rel="noopener" target="_blank">uniquecolors.py</a> to generate any arbitrary number of unique colors, which I used to create the above clip.

The actual sliding window search is facilitated by the <code>slidingWindow()</code> helper function defined in the file <a href="https://github.com/nrsyed/svm-vehicle-detector/blob/master/slidingwindow.py" rel="noopener" target="_blank">slidingwindow.py</a>. The function has the following signature:

<pre class="line-numbers" data-start=3><code class="language-python">def slidingWindow(image_size, init_size=(64,64), x_overlap=0.5, y_step=0.05,
        x_range=(0, 1), y_range=(0, 1), scale=1.5)</code></pre>

Note that the function doesn't actually operate on an image. Instead, we feed it <code>image_size</code>, a tuple containing the width and height of an image, from which it determines the coordinates of all the windows to search (based on the remaining input parameters). This allows us to call the function once, store the list of window coordinates to be searched, then refer to this list for every frame.

As demonstrated in the video, the window traverses the image from top to bottom and left to right. <code>init_size</code> sets the initial size of the window. <code>x_overlap</code> sets the overlap between adjacent windows while moving left to right, as a fraction of the current window width; in other words, if the current window width were 100 pixels, <code>x_overlap=0.5</code> would cause the window to step 50 pixels to the right for each left-to-right step. <code>y_step</code> determines the amount by which the top of the window slides down for each step in the top-to-bottom direction, as a fraction of total image height. If the image were 600 pixels tall, <code>y_step=0.05</code> would cause the window to step 30 pixels toward the bottom at each vertical step. <code>x_range</code> and <code>y_range</code> set the portion of the image to search as a fraction of the image width and height, respectively. For example, <code>x_range=(0, 0.5)</code> would cause only the left half of the image to be searched. Similarly, <code>y_range=(0.67, 1.0)</code> would cause only the bottom third of the image to be searched. Finally, <code>scale</code> sets the ratio by which to increase the size of the window with each vertical step toward the bottom of the image. If <code>scale > 1</code>, the window gets larger with each step in the y direction. If <code>scale < 1</code>, the window gets smaller with each step in the y direction. If <code>scale = 1</code>, the window size remains fixed.

The actual implementation of these parameters is fairly short:

<pre class="line-numbers" data-start=35><code class="language-python">    windows = []
    h, w = image_size[1], image_size[0]
    for y in range(int(y_range[0] * h), int(y_range[1] * h), int(y_step * h)):
        win_width = int(init_size[0] + (scale * (y - (y_range[0] * h))))
        win_height = int(init_size[1] + (scale * (y - (y_range[0] * h))))
        if y + win_height > int(y_range[1] * h) or win_width > w:
            break
        x_step = int((1 - x_overlap) * win_width)
        for x in range(int(x_range[0] * w), int(x_range[1] * w), x_step):
            windows.append((x, y, x + win_width, y + win_height))

    return windows</code></pre>

Observe that each window is added to the list in the form of a tuple containing the x and y coordinates of the window's upper left corner and the x and y coordinates of the window's lower right corner, in that order.

<h2>Building the detector</h2>
Having established our sliding window function, we can move on to making use of it in the object detector, for which I've defined a <code>Detector</code> class in a file aptly named <a href="https://github.com/nrsyed/svm-vehicle-detector/blob/master/detector.py" rel="noopener" target="_blank">detector.py</a>. The <code>__init__()</code> method of this class, found on <b>lines 19-30</b>, sets the sliding window parameters we just discussed in the previous section. Next, the classifier dictionary produced by the <code>trainSVM()</code> function in <a href="https://github.com/nrsyed/svm-vehicle-detector/blob/master/train.py" rel="noopener" target="_blank">train.py</a> is loaded via the <code>loadClassifier()</code> method on <b>lines 32-75</b>. From the dict, we extract the scikit-learn <code>LinearSVC</code> on <b>line 50</b> (which actually classifies feature vectors as containing or not containing the object on which it was trained), the scikit-learn <code>StandardScaler</code> on <b>line 51</b> (which scales feature vectors before they're fed to the classifier), and the color space and color channels for which we wish to extract features on <b>lines 52-53</b>:

<pre class="line-numbers" data-start=50><code class="language-python">        self.classifier = classifier_data["classifier"]
        self.scaler = classifier_data["scaler"]
        self.cv_color_const = classifier_data["cv_color_const"]
        self.channels = classifier_data["channels"]</code></pre>

Note that we use an <a href="https://docs.opencv.org/3.1.0/d7/d1b/group__imgproc__misc.html#ga4e0972be5de079fed4e3a10e24ef5ef0" rel="noopener" target="_blank">OpenCV color space conversion constant</a>, which was determined by the <code>processFiles()</code> function in train.py. Since OpenCV uses the BGR color space by default and no color conversion is required if the user has chosen to use this color space, I've assigned -1 as a default value, which doesn't correspond to an actual OpenCV color conversion constant.

Then we re-instantiate a <code>Descriptor</code>, which will produce the feature vector for each window, using the original descriptor parameters on <b>lines 59-73</b>. Originally, I'd packaged the <code>Descriptor</code> object into the dictionary, but found that, if the dictionary was pickled (saved to file), attempting to load the pickle file produced errors if the pickled dictionary included a <code>Descriptor</code>.

The last helper method we define for the <code>Detector</code> class is <code>classify()</code> on <b>lines 77-99</b>. The signature for the function is simply:

<pre class="line-numbers" data-start=77><code class="language-python">    def classify(self, image):</code></pre>

It takes an image in the form of a 3D numpy array, converts it to the appropriate color space (<b>lines 84-85</b>) and keeps only the desired channels (<b>lines 87-90</b>):

<pre class="line-numbers" data-start=84><code class="language-python">        if self.cv_color_const > -1:
            image = cv2.cvtColor(image, self.cv_color_const)

        if len(image.shape) > 2:
            image = image[:, :, self.channels]
        else:
            image = image[:, :, np.newaxis]</code></pre>

We check that the array is three-dimensional even if it contains only a single channel, adding a third dimension via <code>np.newaxis</code> on <b>line 90</b> if necessary. Having preprocessed the image, the next step is to obtain the feature vector for each window from the sliding window (<b>lines 92-94</b>), which we store in a list called <code>feature_vectors</code>.

<pre class="line-numbers" data-start=92><code class="language-python">        feature_vectors = [self.descriptor.getFeatureVector(
                image[y_upper:y_lower, x_upper:x_lower, :])
            for (x_upper, y_upper, x_lower, y_lower) in self.windows]</code></pre>

Then we scale the feature vectors and run them through the SVM classifier, which returns an array of 0s and 1s, where each element corresponds to a window, 0 signifies that the window does not contain the object, and 1 signifies that it does. Lastly, on <b>line 99</b>, we use a list comprehension to return only the window coordinates of windows predicted to contain an object.

<pre class="line-numbers" data-start=97><code class="language-python">        feature_vectors = self.scaler.transform(feature_vectors)
        predictions = self.classifier.predict(feature_vectors)
        return [self.windows[ind] for ind in np.argwhere(predictions == 1)[:,0]]</code></pre>

We're now ready to actually apply the sliding window search and classification to a video, which will be the topic of the next post.
