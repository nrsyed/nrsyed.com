This post is part of a series on developing an SVM classifier to find vehicles in a video:

<a href="https://nrsyed.com/2018/05/06/hog-based-svm-for-detecting-vehicles-in-a-video-part-1/">Part 1: SVMs, HOG features, and feature extraction</a>
Part 2: Sliding window technique and heatmaps
<a href="https://nrsyed.com/2018/05/10/hog-based-svm-for-detecting-vehicles-in-a-video-part-3/">Part 3: Feature descriptor code and OpenCV vs scikit-image HOG functions</a>
<a href="https://nrsyed.com/2018/05/16/hog-based-svm-for-detecting-vehicles-in-a-video-part-4/">Part 4: Training the SVM classifier</a>
<a href="https://nrsyed.com/2018/05/19/hog-based-svm-for-detecting-vehicles-in-a-video-part-5/">Part 5: Implementing the sliding window search</a>
<a href="https://nrsyed.com/2018/05/24/hog-based-svm-for-detecting-vehicles-in-a-video-part-6/">Part 6: Heatmaps and object identification</a>

The code is also available on <a href="https://github.com/nrsyed/svm-vehicle-detector" rel="noopener" target="_blank">Github</a>. In this post, we'll examine the portion of the vehicle detection pipeline after the SVM has been trained. Once again, here's the final result:

<iframe width="800" height="450" src="https://www.youtube.com/embed/uOxkAF0iA3E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h2>The sliding window technique</h2>
The goal of our vehicle detection SVM is to classify an image as "vehicle" or "non-vehicle". For this project, I trained the SVM on 64x64 patches that either contained a vehicle or didn't contain a vehicle. A blown-up example of each (as well as its HOG visualization) is presented again below:

<img src="https://nrsyed.com/wp-content/uploads/2018/05/hog_visualization.png" alt="Visualizing histogram of oriented gradient features for an image of a vehicle and an image of a non-vehicle (road)" width="800" height="800" class="aligncenter size-full wp-image-608" />

These images were obtained from the <a href="https://github.com/udacity/CarND-Vehicle-Detection" rel="noopener" target="_blank">Udacity dataset</a>, which contained 8799 64x64 images of vehicles and 8971 64x64 images of non-vehicles, all cropped to the correct size. These images are actually subsets of the <a href="http://www.gti.ssr.upm.es/data/Vehicle_database.html" rel="noopener" target="_blank">GTI</a> and <a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" rel="noopener" target="_blank">KITTI</a> vehicle datasets, if you'd like to check those out.

However, actual frames of a video won't be nicely cropped 64x64 images. Instead, they'll look something like this:
<img src="https://nrsyed.com/wp-content/uploads/2018/05/example_frame-1024x576.png" alt="Example frame from the test video." width="840" height="473" class="alignleft size-large wp-image-615" />

To find vehicles in a full image, we utilize a sliding window. Traditionally, this involves selecting a window of a relatively small size, like 64x64 or 100x100, etc., and "sliding" it across the image in steps until the entirety of the image has been covered. Each window is fed to the SVM, which classifies that particular window as "vehicle" or "non-vehicle". This is repeated at several scales of the image by scaling the image down. Sampling the image at different sizes helps ensure that instances of the object both large and small will be found. For example, a vehicle that's nearby will appear larger than one that's farther away, so downscaling improves the odds that both will be detected. This repeated downscaling of the image is referred to as an "image pyramid." Adrian Rosebrock has <a href="https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/" rel="noopener" target="_blank">a good set of posts on using sliding windows and image pyramids for object detection over at pyimagesearch</a>.

In my implementation, I originally utilized a sliding window of fixed size sampling different scales of the image via an image pyramid, but found it to produce many false positives. As an alternative, I tried a sliding window of variable size with a relatively small initial size that increased in size with each step toward the bottom of the image, the rationale being that vehicles near the bottom of the image are physically closer to the camera and appear larger (and vice versa). Furthermore, I only scanned the portion of the image below the horizon, since there shouldn't be any vehicles above the horizon. This seemed to improve the results significantly. The following short clip demonstrates the technique:

<iframe width="800" height="450" src="https://www.youtube.com/embed/6e8XB-bKsZ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

Observe that there's considerable overlap between windows:

<img src="https://nrsyed.com/wp-content/uploads/2018/05/sliding_window_example-1024x576.png" alt="Example of variable-sized sliding window applied to a video frame from the horizon down." width="840" height="473" class="aligncenter size-large wp-image-617" />

This means that the same vehicle will probably be detected multiple times in adjacent windows, which ends up looking like this:

<img src="https://nrsyed.com/wp-content/uploads/2018/05/sliding_window_detections-1024x576.png" alt="Overlapping detections of the same car, after applying the sliding window technique" width="840" height="473" class="aligncenter size-large wp-image-618" />

The full video, with all detections drawn at every frame:

<iframe width="800" height="450" src="https://www.youtube.com/embed/TfX6jtuPL0I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

However, our goal is to find (and draw a box around) individual vehicles. To do this, we must reduce multiple overlapping detections into a single detection, which we'll explore in the next section.

<strong>IMPORTANT NOTE ON WINDOW SIZES:</strong> Recall that the features extracted from each window will be fed to and classified by the SVM (as "positive" / "vehicle" or "negative" / "non-vehicle"). If the SVM was trained on 64x64 images, the features extracted from each window must also come from a 64x64 image. If the window is a different size, then each window should be resized to 64x64 before feature extractionâ€”otherwise, the results will be meaningless.

<h2>Heatmap for combining repeated detections</h2>
Several methods exist for reducing repeated detections into a single detection. One approach is to build a heatmap in which each detection contributes some positive value to the region of the image it covers. Regions of the image with many overlapping detections will provide a greater number of contributions to that region of the heatmap, seen in the image below (the full-scale heatmap superimposed on the raw image uses shades of red instead of grayscale like the small-scale inset heatmap in the corner):

<img src="https://nrsyed.com/wp-content/uploads/2018/05/heatmap_overlay-1024x576.png" alt="Full-scale heatmap superimposed on the original video (in red)" width="840" height="473" class="aligncenter size-large wp-image-622" />

To reduce false positives, we can use the weighted sum and/or average of the heatmaps of the last N frames of the video, giving a higher weight to more recent frames and a lower weight to older frames, then consider only areas of the summed heatmap with values above some threshold. This way, regions with consistent and sustained detections are more likely to be correctly identified as vehicles, while areas with fleeting detections that only last a couple frames (which are likely false positives) are more likely to be ignored. Here's the complete video with the full-scale superimposed heatmap:

<iframe width="800" height="450" src="https://www.youtube.com/embed/OT68xGggpkM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h2>Labeling and boxing distinct objects</h2>

Having combined multiple detections into "blobs" on the heatmap, we can now apply a connected-component algorithm to determine how many distinct objects (blobs) are present, as well as which pixels belong to which object:

<img src="https://nrsyed.com/wp-content/uploads/2018/05/labeled_objects-1024x576.png" alt="&quot;Blobs&quot; from the heatmap labeled as individual objects." width="840" height="473" class="aligncenter size-large wp-image-629" />

And, of course, the full video with each object labeled with a unique identifier (number):

<iframe width="800" height="450" src="https://www.youtube.com/embed/iOt_3tQJBFY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

This video demonstrates an important fact: objects are numbered arbitrarily and sometimes change numbers between frames. In other words, this algorithm detects objects but does <em>not</em> track them.

The final step, after finding each object, is to draw the largest possible bounding box around each object, ignoring objects that are too small and likely to be false positives.

<img src="https://nrsyed.com/wp-content/uploads/2018/05/final_bounding_boxes-1024x576.png" alt="Final result, in which each vehicle is correctly identified once." width="840" height="473" class="aligncenter size-large wp-image-623" />

In the next post, we'll delve into finer implementation details and specifics by examining the code.
