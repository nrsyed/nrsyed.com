The <a href="https://nrsyed.com/2018/03/25/image-segmentation-via-k-means-clustering-to-decipher-color-blindness-tests/">previous post</a> discussed the use of K-means clustering and different color spaces to isolate the numbers in Ishihara color blindness tests:

<img src="https://nrsyed.com/wp-content/uploads/2018/03/ishihara_42_ycc_c1n2.jpg" alt="" width="618" height="300" class="aligncenter size-full wp-image-581" />

In the figure above, the original image on the left was converted to the YCrCb color space, after which K-means clustering was applied to the Cr channel to group the pixels into two clusters. The result is the image on the right, where black represents one cluster and white represents the other cluster.

In this post, we'll go through the Python code that produced this figure (and the other figures from the previous post) using OpenCV and scikit-learn.

<h1>The code</h1>
The <a href="https://github.com/nrsyed/computer-vision/blob/master/kmeans_color_segmentation/color_segmentation.py" rel="noopener" target="_blank">script can be found on my github</a>, if you're so inclined. Otherwise, fire up a text editor and create a file named <code>color_segmentation.py</code>.

<pre class="line-numbers"><code class="language-python">import numpy as np
from sklearn.cluster import KMeans
import argparse
import cv2
import datetime</code></pre>
First, the necessary imports. The <code>datetime</code> module will be used to construct a unique timestamped filename for the output image.

<pre class="line-numbers" data-start=7><code class="language-python">ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help='Path to image file')
ap.add_argument('-w', '--width', type=int, default=0,
    help='Width to resize image to in pixels')
ap.add_argument('-s', '--color-space', type=str, default='bgr',
    help='Color space to use: BGR (default), HSV, Lab, YCrCb (YCC)')
ap.add_argument('-c', '--channels', type=str, default='all',
    help='Channel indices to use for clustering, where 0 is the first channel,'
    + ' 1 is the second channel, etc. E.g., if BGR color space is used, "02" '
    + 'selects channels B and R. (default "all")')
ap.add_argument('-n', '--num-clusters', type=int, default=3,
    help='Number of clusters for K-means clustering (default 3, min 2).')
ap.add_argument('-o', '--output-file', action='store_true',
    help='Save output image (side-by-side comparison of original image and'
    + ' clustering result) to disk.')
ap.add_argument('-f', '--output-format', type=str, default='png',
    help='File extension for output image (default png)')</code></pre>
Next, we construct the argument parser to handle input options and parameters. The usage of each flag can be seen from its respective help text above and should also become more clear as we go through the rest of the code.

<pre class="line-numbers" data-start=25><code class="language-python">args = vars(ap.parse_args())
image = cv2.imread(args['image'])

# Resize image and make a copy of the original (resized) image.
if args['width'] > 0:
    height = int((args['width'] / image.shape[1]) * image.shape[0])
    image = cv2.resize(image, (args['width'], height),
        interpolation=cv2.INTER_AREA)
orig = image.copy()</code></pre>
<strong>Line 25</strong> puts the input arguments into a dictionary <code>args</code>. <strong>Line 26</strong> reads the input image and stores it in <code>image</code>. If a width was specified by the user, <strong>lines 29-32</strong> resize the image using the OpenCV function <code>resize()</code>. At this stage, we create a copy of the image on <strong>line 33</strong> since we'll continue to modify the image (this allows us to use or display the original later).

<pre class="line-numbers" data-start=35><code class="language-python"># Change image color space, if necessary.
colorSpace = args['color_space'].lower()
if colorSpace == 'hsv':
    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
elif colorSpace == 'ycrcb' or colorSpace == 'ycc':
    image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)
elif colorSpace == 'lab':
    image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
else:
    colorSpace = 'bgr'  # set for file naming purposes</code></pre>
Next, the image is converted to the desired color space, if the user specified one, using OpenCV's <code>cvtColor()</code> function. Note that OpenCV utilizes the BGR color space by default, not RGB, when it reads in an image with <code>cv2.imread()</code> or displays a color image with <code>cv2.imshow()</code>.

<pre class="line-numbers" data-start=46><code class="language-python"># Keep only the selected channels for K-means clustering.
if args['channels'] != 'all':
    channels = cv2.split(image)
    channelIndices = []
    for char in args['channels']:
        channelIndices.append(int(char))
    image = image[:,:,channelIndices]
    if len(image.shape) == 2:
        image.reshape(image.shape[0], image.shape[1], 1)</code></pre>
Now that the image has been converted to the correct color space, we'll isolate only the channels on which we wish to perform K-means clustering. The <code>channels</code> input argument takes a string of digits, where each digit represents a channel index. For example, in the BGR color space, channel 0 would be B (the blue channel), 1 would be G (the green channel), and 2 would be R (the red channel). Similarly, if we'd converted the image to the YCrCb color space, 0 would denote channel Y (luma), 1 would denote channel Cr (red-difference), and 2 would denote channel Cb (blue-difference). For example, a user input of "01" would mean we wish to use channels 0 and 1 for K-means clustering. An input of "2" would mean we wish to use only channel 2 for K-means clustering. If only a single channel is selected, the resulting numpy array loses its third dimension (an image array's first index represents the row, its second index represents the column, and the third index represents the channel). <strong>Lines 53-54</strong> check for this and simply "reshape" the array by adding a dummy third index of length 1, if necessary, so the array works with the upcoming code.

<pre class="line-numbers" data-start=56><code class="language-python"># Flatten the 2D image array into an MxN feature vector, where M is
# the number of pixels and N is the dimension (number of channels).
reshaped = image.reshape(image.shape[0] * image.shape[1], image.shape[2])

# Perform K-means clustering.
if args['num_clusters'] < 2:
    print('Warning: num-clusters < 2 invalid. Using num-clusters = 2')
numClusters = max(2, args['num_clusters'])
kmeans = KMeans(n_clusters=numClusters, n_init=40, max_iter=500).fit(reshaped)</code></pre>
The scikit-learn K-means clustering method <code>KMeans.fit()</code> takes a 2D array whose first index contains the samples and whose second index contains the features for each sample. In other words, each row in the input array to this function represents a pixel and each column represents a channel. We achieve this by reshaping the image array on <strong>line 58</strong>. <strong>Lines 61-62</strong> ensure that a value of at least 2 clusters was chosen (since classifying the pixels into a single cluster would be meaningless).

<strong>Line 64</strong> actually applies K-means clustering to the input array. <code>KMeans(n_clusters=numClusters, n_init=40, max_iter=500)</code> creates a KMeans object with the given parameters. <code>n_init=40</code> means that K-means clustering will be run 40 times on the data, with the initial centroids randomized to different locations each time, from which the best result will be returned. 40 isn't some universal magic number—I simply tried a few different values and found it to be the lowest value that provided consistent results. <code>max_iter=500</code> means that, during each of those 40 runs, the cluster centroids will be updated until they stop changing or until the algorithm has continued for 500 iterations. Again, 500 isn't a magic number. I simply found it to provide the most consistent results. <code>fit(reshaped)</code> actually runs the algorithm using these parameters on our dataset.

<pre class="line-numbers" data-start=66><code class="language-python"># Reshape result back into a 2D array, where each element represents the
# corresponding pixel's cluster index (0 to K - 1).
clustering = np.reshape(np.array(kmeans.labels_, dtype=np.uint8),
    (image.shape[0], image.shape[1]))

# Sort the cluster labels in order of the frequency with which they occur.
sortedLabels = sorted([n for n in range(numClusters)],
    key=lambda x: -np.sum(clustering == x))</code></pre>

After running the K-means clustering algorithm, we retrieve the cluster labels using the <code>labels_</code> member array of the <code>KMeans</code> object. We reshape this back into the image's original 2D shape on <strong>lines 68-69</strong>.

Since we're going to display the clustered result as a grayscale image, it makes sense to assign hues (black, white, and as many shades of gray in between as are necessary) to the clusters in a logical order. The cluster labels won't necessarily be the same each time K-means clustering is performed, even if the pixels in the image are grouped into the same clusters—e.g., <code>KMeans.fit()</code> might, on one run, put the pixels of the number in a color blindness test into cluster label "0" and the background pixels into cluster label "1", but running it again might group pixels from the number into cluster label "1" and the background pixels into cluster label "0". However, assuming the pixels are clustered the same way each time (even if the clusters end up with different labels), the total number of pixels in any given cluster shouldn't change between runs. <strong>Lines 72-73</strong> exploit this by putting the cluster labels in a list sorted by the frequency with which they occur in the clustered image, from most to least frequent.

<pre class="line-numbers" data-start=75><code class="language-python"># Initialize K-means grayscale image; set pixel colors based on clustering.
kmeansImage = np.zeros(image.shape[:2], dtype=np.uint8)
for i, label in enumerate(sortedLabels):
    kmeansImage[clustering == label] = int((255) / (numClusters - 1)) * i</code></pre>
Finally, we create a single-channel 8-bit grayscale image where each pixel is assigned a hue based on the cluster to which it belongs and the frequency of the cluster (from the sorted list of labels). In an 8-bit grayscale image, a pixel with a value of 0 is black and a pixel with a value of 255 is white, which is where 255 comes from on <strong>line 78</strong>. At this point, we could stop and display the grayscale image with a call to <code>cv2.imshow()</code>. For convenience, though, let's put the original image and the resulting clustered image, <code>kmeansImage</code>, side by side:

<pre class="line-numbers" data-start=80><code class="language-python"># Concatenate original image and K-means image, separated by a gray strip.
concatImage = np.concatenate((orig,
    193 * np.ones((orig.shape[0], int(0.0625 * orig.shape[1]), 3), dtype=np.uint8),
    cv2.cvtColor(kmeansImage, cv2.COLOR_GRAY2BGR)), axis=1)
cv2.imshow('Original vs clustered', concatImage)</code></pre>
Since we want to show the original BGR image alongside the clustered grayscale image, we have to convert the grayscale image to BGR. For clarity, I've opted to add a strip of gray between the two images, whose hue is given by the arbitrary value 193, whose height is the same as that of the images, and whose width I've defined as a percentage (6.25%) of the image width. The original image, gray divider, and clustered image are stacked horizontally with a call to the numpy function <code>concatenate()</code>.

<pre class="line-numbers" data-start=86><code class="language-python">if args['output_file']:
    # Construct timestamped output filename and write image to disk.
    dt = datetime.datetime.now()
    fileExtension = args['output_format']
    filename = (str(dt.year) + str(dt.month) + str(dt.day) + str(dt.hour)
        + str(dt.minute) + str(dt.second) + colorSpace + '_c' + args['channels']
        + 'n' + str(numClusters) + '.' + fileExtension)
    cv2.imwrite(filename, concatImage)
cv2.waitKey(0)</code></pre>
Lastly, if the user supplied the <code>--output-file</code> option, <strong>lines 86-93</strong> construct a timestamped filename that contains the name of the chosen color space, the channel indices, and the number of clusters used for clustering, then write the image to disk with a call to <code>cv2.imwrite()</code>. <strong>Line 94</strong> waits until any key is pressed to close the window previously displayed by <code>cv2.imshow()</code>.

<h1>Usage</h1>
Say we had the following source image, named <code>ishihara_5_original.jpg</code>:

<img src="https://nrsyed.com/wp-content/uploads/2018/03/ishihara_5_original.jpg" alt="" width="300" height="300" class="aligncenter size-full wp-image-566" />

We might run the following in our terminal or command line window:

<pre><code class="language-bash">python color_segmentation.py -i ishihara_5_original.jpg -w 300 -s hsv -c 02 -n 3 -o -f jpg</code></pre>

This translates to "resize the source image ishihara_5_original.jpg (<code>-i ishihara_5_original.jpg</code>) to a width of 300 pixels (<code>-w 300</code>), convert it to the HSV color space (<code>-s hsv</code>), then perform K-means clustering on channels 0 and 2 (<code>-c 02</code>) by grouping the pixels into 3 clusters (<code>-n 3</code>) and output the resulting file (<code>-o</code>) in JPG format (<code>-f jpg</code>)." The result would look like this:

<img src="https://nrsyed.com/wp-content/uploads/2018/03/ishihara_5_hsv_c02n3.jpg" alt="" width="618" height="300" class="aligncenter size-full wp-image-575" />

The resulting filename of the output image would be something like <code>2018329194036hsv_c02n3.jpg</code>. Recall from the beginning of the file that all arguments except the source image filename are optional, though the defaults won't necessarily provide optimal clustering. For example, this is what happens if we just run it with the default settings—BGR color space (equivalent to specifying the option <code>-s bgr</code>), all channels (equivalent to specifying <code>-c 012</code> or <code>-c all</code>), and 3 clusters:

<pre><code class="language-bash">python color_segmentation.py -i ishihara_5_original.jpg -w 300</code></pre>

<img src="https://nrsyed.com/wp-content/uploads/2018/03/ishihara_rgb_c012_n3.jpg" alt="Ishihara test and its RGB K-means clustered counterpart" width="618" height="300" class="aligncenter size-full wp-image-567" />

Try the script on your own images, or tweak it to your liking.
